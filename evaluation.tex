\section{Evaluation}
\label{sec.evaluation}

To evaluate Lind's effectiveness,
%in containing untrusted code and protecting the OS kernel,
we compared its performance against seven existing
virtualization systems -- VirtualBox, VMWare
Workstation, Docker, LXC, QEMU, KVM and Graphene.
%This section describes
%the purpose and setup of our experiments, and presents and discusses our results.
We chose these seven systems because they are representative of the most
widely-used current design models.
VirtualBox and VMWare Workstation are commercial OS VM products, while
LXC and KVM are designed for the Linux kernel. Docker, QEMU, and Graphene are well-known open
source projects. Lastly, we also tested Native Linux to serve as a
baseline for comparison.
%
Our tests were designed to answer four fundamental questions:

\textit{How does Lind compare to other virtualization systems
in protecting against zero-day Linux kernel bugs?}
(Section~{\ref{Linux-Kernel-Bug-Test-and-Evaluation}})

\textit{How much of the underlying kernel code is exposed, and is thus
vulnerable in different virtualization systems?}
(Section~{{\ref{Reachable-Kernel-Trace-Analysis-for-Different-Virtualization-Systems}})

\textit{If Lind's SafePOSIX construction has bugs, how much threat does this pose?}
(Section~{{\ref{Reachable-Kernel-Trace-Analysis-for-Repy-Sandbox}})

\textit{In the Lind prototype, what is the expected performance overhead in
real-world applications?}
(Section~{{\ref{Performance-Evaluation}})

\subsection{Linux Kernel Bug Test and Evaluation}
\label{Linux-Kernel-Bug-Test-and-Evaluation}

%\textbf{Test Purpose.}
%To evaluate how well each virtualization system protects the Linux kernel
%against reported zero-day bugs.
%\brendan{This is my best understanding of how the tests were conducted. Yiwen et al., please jump in and correct me if I got something wrong.} 
%\yiwen{The setup section looks good to me.}

\noindent
\textbf{Setup.}
To evaluate how well each virtualization system protects the Linux kernel
against reported zero-day bugs,
we identified and examined a list of 69 historical bugs that were found and patched in version 3.14.1 the Linux kernel \cite{CVE-Datasource}.
By analyzing security patches for those bugs,
we identified the lines of code in the kernel that correspond to each one.

In the following evaluation, we assume that a bug is potentially triggerable if the lines of code that were changed in the patch are reached 
(i.e., the same metric described in Section~\ref{sec.metric}).
This measure may overestimate potential danger posed by a system (since simply reaching the buggy code does not mean that guest code 
actually has enough control to exploit the bug).
However, this overestimate should apply equally to all of the systems we tested, which means it is still useful as a way to compare them.

Next, we sought out proof-of-concept code that triggered each bug. We were able to obtain or create code that triggered nine out of the 69 bugs \cite{Exploit-Database}.
For the remaining, we used the Trinity system call fuzzer \cite{Trinity} on Linux 3.14.1 (referred to as ``Native'' Linux in Table~\ref{table:trigger_vulnerabilities}).
By comparing the code reached during fuzzing with the lines of code affected by security patches, we were able to identify an additional 26 bugs that could be triggered.

We then evaluated the protection afforded by eight virtualization systems (including Lind) by attempting to trigger the 35 bugs from inside each.
The host system for each test ran version of Linux 3.14.1 with gcov instrumentation enabled.
For the nine bugs that we could directly trigger, we ran the proof of concept exploit inside the guest.
For the other 26, we ran the Trinity fuzzer inside the guest, exercising each system call 1,000,000 times with random inputs.
Finally, we checked whether the lines of code containing each bug were reached in the host kernel, indicating that the guest could have triggered the bug.

%To test if a bug could be triggered, we created or located C
%code capable of exploiting each kernel bug \cite{Exploit-Database}.
%We were only able to trigger and obtain results for 35 out of the 69 bugs in our
%experiments. In some cases, there was a
%difficulty in clearly determining if triggering had occurred, while in other we
%were unable to find code to trigger them. We decided to focus our study on
%this group of 35 bugs and leave the more complex ones to future work.

%We compiled and ran the exploit C code under each virtualization system to
%obtain their kernel traces, and then used our kernel trace safety metric to
%determine if a specific bug was triggered.

\begin{table*}[!ht]
\scriptsize
\centering

\begin{tabular}{|p{1.7cm}|l|l|p{1cm}|p{1cm}|p{.8cm}|p{1cm}|p{.8cm}|p{1cm}|p{.8cm}|}\hline

\multirow{2}{1.7cm}{\bf Vulnerability}    &  \multirow{2}{.7cm}{\bf Native Linux}  &  \multirow{2}{*}{\bf VirtualBox}
&  \multirow{2}{.7cm}{\bf VMWare}
 & \multirow{2}{1cm}{\bf Docker} & \multirow{2}{1cm}{\bf LXC} &
\multirow{2}{1cm}{\bf QEMU} & \multirow{2}{1cm}{\bf KVM} &
\multirow{2}{1cm}{\bf Graphene} & \multirow{2}{1cm}{\bf Lind} \\
& & & & & & & & & \\
\hline

 CVE-2015-5706 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{blue}\ding{51}+g} &
\multirow{1}{1cm}{{\color{blue}\ding{51}+g}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\ding{55}  \\

 CVE-2015-0239 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \ding{55} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-9584 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55}& \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-9529 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}}  &
\ding{55}  & \ding{55} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\ding{55}& \ding{55} &
\ding{55}  & \ding{55}  \\

 CVE-2014-9322 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}}  &
\ding{55}  & \multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}}  & \ding{55}
\\

 CVE-2014-9090 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-8989 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\ding{55}  \\

 CVE-2014-8559 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-8369 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-8160 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \ding{55} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\ding{55} & \ding{55} &
\ding{55}  & \ding{55}  \\

 CVE-2014-8134 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \ding{55} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\ding{55} & \ding{55} & \multirow{1}{1cm}{{\color{red}\ding{51}}}  & \ding{55}
\\

 CVE-2014-8133 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}}  &
\ding{55}  & \ding{55} & \ding{55} &
\ding{55} & \ding{55} &
\ding{55}  & \ding{55}  \\

 CVE-2014-8086 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{blue}\ding{51}+g} &
\multirow{1}{1cm}{{\color{blue}\ding{51}+g}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} &
\ding{55} & \ding{55} &
\ding{55} & \ding{55}  \\

 CVE-2014-7975 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-7970 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-7842 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-7826 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \ding{55} & \ding{55}  &
\ding{55} & \ding{55} & \multirow{1}{1cm}{{\color{red}\ding{51}}}  & \ding{55}
\\

 CVE-2014-7825 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \ding{55} & \ding{55} &
\ding{55} & \ding{55} & \multirow{1}{1cm}{{\color{red}\ding{51}}}  & \ding{55}
\\

 CVE-2014-7283 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-5207 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-5206 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
\multirow{1}{1cm}{{\color{red}\ding{51}}}  & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} &
\ding{55} & \ding{55} &
\ding{55}  & \ding{55}
\\

 CVE-2014-5045 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-4943 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-4667 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} & \multirow{1}{1cm}{{\color{red}\ding{51}}}  & \ding{55}  \\

 CVE-2014-4508 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-4171 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}}  \\

 CVE-2014-4157 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-4014 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
\multirow{1}{1cm}{{\color{red}\ding{51}}}  & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\multirow{1}{1cm}{{\color{red}\ding{51}}} &
\ding{55} & \ding{55} &
\ding{55}  & \ding{55}
\\

 CVE-2014-3940 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{blue}\ding{51}+g}  &
\ding{55}  & \multirow{1}{1cm}{{\color{red}\ding{51}}} & \multirow{1}{1cm}{{\color{red}\ding{51}}} &
\ding{55} & \ding{55} &
\ding{55}  & \ding{55}  \\

 CVE-2014-3917 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & {\color{red}\ding{51}}  &
\ding{55}  & \ding{55} & \ding{55} &
\ding{55} & \ding{55} &
\ding{55}  & \ding{55}  \\

 CVE-2014-3153 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
  \ding{55}  & \ding{55} & \ding{55} &
  \ding{55} & \ding{55} &
  \ding{55}  & \ding{55}  \\

 CVE-2014-3144 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-3122 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-2851 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &
 \ding{55}  & \ding{55}  \\

 CVE-2014-0206 & \multirow{1}{.7cm}{{\color{red}\ding{51}}} & \ding{55}  &
 \ding{55}  & \ding{55} & \ding{55} &
 \ding{55} & \ding{55} &  \ding{55}  & \ding{55}  \\
\hline

 {\bf Vulnerabilities Triggered} & \multirow{2}{1cm}{\bf 35/35 (100\%)} & {\bf 14/35 (40.0\%)} &
 {\bf 11/35 (31.4\%)}  & {\bf 8/35 (22.9\%)} & {\bf 12/35 (34.3\%)} & {\bf 5/35 (14.3\%)} & {\bf 5/35 (14.3\%)} &
 {\bf 8/35 (22.9\%)}  & {\bf 1/35 (2.9\%)}  \\
\hline
\end{tabular}

\caption {\small Linux Kernel Bugs, and Vulnerabilities in Different Virtualization Systems
({\color{red}\ding{51}}: vulnerability triggered;
{\color{blue}\ding{51}+g}: vulnerability triggered with guest additions; \ding{55}: vulnerability
not triggered).}

\label{table:trigger_vulnerabilities}
\end{table*}

\noindent
\textbf{Results.}
We found that a substantial number of bugs could be triggered in existing
virtualization systems, as shown in Table \ref{table:trigger_vulnerabilities}.
All (100\%) bugs were triggered in Native Linux,
while the other programs had lower rates: 14/35 (40\%) in
VirtualBox,
11/35 (31.4\%)  in VMWare Workstation, 8/35 (22.9\%)  in Docker,
12/35 (34.3\%)  in LXC, 5/35 (14.3\%)  in QEMU, 5/35 (14.3\%)  in KVM,
and 8/35 (22.9\%) bugs in Graphene.
In comparison, only 1 out of 35 bugs  (2.9\%) was triggered in Lind.

To better understand how the behaviors of bugs varied
from system to system, we present brief case studies of four
vulnerabilities from Table \ref{table:trigger_vulnerabilities}.
These
studies demonstrate how the implementation of a system design can
affect security in different ways.

\emph{\textbf{All systems vulnerable.}}  Representative bug: CVE-2014-4171.
This is the only vulnerability that was triggered in every
system, including Lind. It resides inside the \texttt{mm/shmem.c} kernel
path (a popular path) that
can be triggered by using \texttt{mmap()} to access a hole in the memory.
The \texttt{mmap()} call then invokes \texttt{shmem\_fault()}, which will cause contention
on \texttt{i\_mmap\_mutex}, and lead to a serious starvation of memory resources.

It is our belief that Lind triggered this bug because \texttt{mmap()} cannot easily
be safely re-created inside our SafePOSIX API. The call sets up a
memory region where the OS will later
intervene and automatically convert all accesses into ones that reach the
underlying file.  The code does not explicitly make system calls, and as
a result, with Lind's design we cannot intercept those accesses and call through
the Repy sandbox kernel. Thus,
Lind allows \texttt{mmap()} calls to directly access the kernel, which
opens the chance to trigger this vulnerability.

Graphene, similar to Lind, does not re-create the
\texttt{mmap()} system call in its library module \texttt{libLinux}. Instead, when doing
memory allocation operations like \texttt{malloc()} and \texttt{brk()}, Graphene will
pass those system calls to the underlying kernel, and rely on it to
perform \texttt{mmap()}.
KVM and LXC both reside inside the Linux kernel and inherit \texttt{mmap()}
from it. QEMU is built on KVM, and so also relies on the KVM module to perform
\texttt{mmap()}. Similarly, Docker, built on LXC, relies on the native Linux kernel to perform
\texttt{mmap()}. In VirtualBox and VMWare Workstation, the OS VM has its own mechanism
for memory management. During the initial setup, the OS VM needs to request memory resource
and allocate memory space by making \texttt{mmap()} to the host kernel. Additionally, during the runtime of the VM process,
when there is excessive memory usage, or reconfiguration of memory allocation, the VM kernel module
will attempt to call \texttt{mmap()} to the host kernel. Thus, this \texttt{mmap()} vulnerability
is exploitable under all the virtualization systems that we tested.

\emph{\textbf{Only Native Linux vulnerable.}}  Representative bug: CVE-2014-5045.
This vulnerability was only triggered in Native Linux. It resides in the
\texttt{fs/namei.c} kernel path (an unpopular path) and was triggered because
the \texttt{mountpoint\_last()}
%\texttt{mountpoint\_last(struct nameidata *nd, struct path *path)}
function does not properly
maintain a reference count during attempts to use the \texttt{umount()} system call
in conjunction with a symbolic link. Unmounting from a symbolic link could block
another unmount operation, and allow attackers to cause a denial of service or
deploy use-after-free exploitation.

Lind does not implement symbolic links, since it is not required to run most applications. 
But even similar functionality that might be risky is implemented entirely
within its SafePOSIX module. The bug would (at most) enable an attacker to execute
code in the Repy sandbox and would not harm other parts of the system.

Graphene does not implement symbolic links in its Linux system call API, and thus avoids
this problem.
OS VM like VirtualBox and VMWare Workstation have their own metadata to maintain their file
directories and symbolic links, which do not directly rely on the host OS.
%\cappos{Why did the flag in the first example
%work then?  Also, why doesn't this exist in Docker / LXC?}
Furthermore, symbolic links in those systems are contained within the virtualization system's image,
and are not able to reach the underlying host OS. In this case, those virtualization systems provide enough
isolation to prevent this bug.
Docker/LXC and QEMU/KVM happen to avoid this bug because their kernel module modified the Linux kernel
that fixed this problem. It should be noted that by their design model, they both tend to suffer from
the same vulnerabilities that exist in Native Linux.

\emph{\textbf{Some systems safe, some systems vulnerable.}}  Representative bug: CVE-2014-8086.
This vulnerability was not triggered in Graphene or Lind, but was triggered in
VirtualBox, VMWare workstation, Docker, and Native Linux. It was one of several
bugs (e.g., CVE-2014-7826 and CVE-2014-5206) that were triggered in just a few systems.
CVE-2014-8086 resides in the \texttt{fs/ext4/file.c} kernel path (an unpopular path), and can be
triggered when a file system \texttt{write}
call is made together with \texttt{fcntl},
using the argument \texttt{F\_SETFL} and the \texttt{O\_DIRECT} flag. If triggered, it could
allow attackers to cause a denial of service (file unavailability), by raising a race condition.
Both Lind and Graphene prevented this bug, but for different reasons. Lind
implements \texttt{fcntl} in SafePOSIX, so the underlying kernel is not called.
Graphene checks and blocks certain system calls, including
\texttt{fcntl} with the \texttt{O\_DIRECT} flag.
\yiwen{More details will be added here. I am still working on it.}
Other systems like VirtualBox, VMWare Workstation, Docker, and Native Linux,
all suffer from this vulnerability because calls go directly into the host OS
kernel.

\emph{\textbf{Only Lind safe.}}  Representative bug: CVE-2015-5706.
This vulnerability was triggered in every virtualization system we tested except Lind,
and is closely related to file system calls and file flags. It resides in the \texttt{fs/namei.c}
kernel path (an unpopular path), and can be triggered by making a \texttt{path\_openat()}
call with flag \texttt{O\_TMPFILE}. \texttt{path\_openat()} will jump to the wrong
place after \texttt{do\_tmpfile()}, and do \texttt{path\_cleanup()} twice. This would
allow local users to perform use-after-free exploitation to cause a denial of service.
This bug was not triggered in Lind, because it does not support the use of the
\texttt{O\_TMPFILE} flag. In fact, the only call in the Repy sandbox that
opens a file does not take an argument for flags or other operations.  The
only arguments allowed are a filename (that must consist of a small number
of highly restricted characters) and a flag to indicate whether a file should
be created if one does not exist.
\yiwen{More details will be added here.}
Other virtualization systems allow more complex configuration of flags to
pass to the underlying OS kernel.
%\cappos{Why does VirtualBox pass this
%through?  My VirtualBox file system is a single VDI file.  How does this end
%up calling into the host OS's kernel and why?}  \cappos{Does this vary
%if you use a different FS type on VirtualBox?}
In this case, the \texttt{O\_TMPFILE} file flag was
allowed in Native Linux, VirtualBox, VMWare Workstation, Docker, and Graphene.

Our initial results % as illustrated by the above case studies,
suggest that bugs are usually triggered by complex system calls, or basic system calls
with complicated or rarely used flags. The \emph{Lock-in-Pop} design, and thus Lind,
provides strictly controlled access to the kernel, and so, poses
the least risk.

\subsection{Comparison of Kernel Code Exposure in Different Virtualization
Systems}
\label{Reachable-Kernel-Trace-Analysis-for-Different-Virtualization-Systems}
\begin{table}
\centering
\scriptsize
\begin{tabular}{|l|l|l|l|l|}
  \hline
  \multirow{3}{1.5cm}{\bf Virtualization system} & \multirow{3}{0.5cm}{\bf \# of Bugs} & \multicolumn{3}{c|}{\bf Kernel trace} \\ \cline{3-5}
  & & \multirow{2}{1.5cm}{Total coverage (LOC)} & \multirow{2}{1.5cm}{In popular paths (LOC)} & \multirow{2}{1.5cm}{In risky paths (LOC)}  \\
  & & & & \\  \hline
  VirtualBox & 14 & 153.0K & 71.1K & 81.9K \\
  \hline
  LXC & 12 & 127.3K & 70.9K & 56.4K \\
  \hline
  \multirow{2}{1.5cm}{VMWare Workstation} & \multirow{2}{*}{11} &
  \multirow{2}{*}{140.9K} & \multirow{2}{*}{70.7K} &\multirow{2}{*}{70.2K} \\
  & & & & \\   \hline
  Docker & 8 & 119.0K & 69.5K & 49.5K \\
  \hline
  Graphene & 8 & 95.5K & 62.2K & 33,3K \\
  \hline
  QEMU & 5 & 136.1K & 83.3K & 52.8K \\
  \hline
  KVM & 5 & 146.4K & 83.4K & 63K \\
  \hline
  Lind & 1 & 70.3K & 70.3K & 0 \\
  \hline
\end{tabular}
\caption{\small Reachable kernel trace analysis for different virtualization
systems.}
\label{table:trace-systems}
\end{table}

%\textbf{Test Purpose.}
%To determine how much of the underlying kernel can be executed and exposed in
%each system. This helps us understand the potential risks a virtualization system
%may pose based upon how much access it allows to the kernel code.

\noindent
\textbf{Setup.}
%To analyze the reachable kernel paths for each
%virtualization system,
To determine how much of the underlying kernel can be executed and exposed in
each system,
we conducted system call fuzzing with Trinity (similar to our approach in
Section~{\ref{sec.metric}}) to obtain
kernel traces. This helps us understand the potential risks a virtualization system
may pose based upon how much access it allows to the kernel code.
All experiments were conducted under Linux kernel 3.14.1.

\noindent
\textbf{Results.}
We obtained the total reachable kernel trace for
each tested system,
and further analyzed the components of those traces. These results,
shown in Table \ref{table:trace-systems}, affirm that Lind accessed the
least amount of code in the OS
kernel. More importantly, all the kernel code it accessed was in the
popular kernel paths that contain fewer bugs (Section~{\ref{Verification-of-Hypothesis}}).
A large portion of the kernel paths accessed by Lind lie in
\texttt{fs/} to perform file system operations.
To restrict file system calls to popular paths, Lind only allows basic calls,
like \texttt{open()}, \texttt{close()}, \texttt{read()}, \texttt{write()}, \texttt{mkdir()},
\texttt{rmdir()}, and commonly used flags like \texttt{O\_CREAT}, \texttt{O\_EXCL},
 \texttt{O\_APPEND}, \texttt{O\_TRUNC},
\texttt{O\_RDONLY}, \texttt{O\_WRONLY}, and \texttt{O\_RDWR} are
permitted for \texttt{open()}.

The other virtualization systems all accessed a substantial number of code
paths in the kernel,
and they all accessed a larger section from the unpopular paths.
This is because they have
more dependence on complex system calls, and
allow extensive use of complicated flags. For example,
Graphene provides a complex system call API that allows
\texttt{fork()} and \texttt{signals}, and can access many risky lines of code.
VirtualBox, and therefore VMWare Workstation, and Docker, have even larger
codebases and more complicated system functions. They allow
rarely used flags, such as \texttt{O\_TMPFILE}, \texttt{O\_NONBLOCK},
and \texttt{O\_DSYNC}, which can reach potentially dangerous lines
of code.

To summarize, our analysis suggests that Lind triggers the fewest kernel bugs because
it has better control over the portions of the OS kernel accessed by applications.

\subsection{Impact of Potential Vulnerabilities in Lind's SafePOSIX Re-creation}
\label{Reachable-Kernel-Trace-Analysis-for-Repy-Sandbox}

%\textbf{Test Purpose.}
%To understand what potential security risks could be posed if Lind's SafePOSIX construction
%has vulnerabilities.

\noindent
\textbf{Setup.}
To understand the potential security risks if Lind's SafePOSIX re-creation
has vulnerabilities, we conducted system call fuzzing with Trinity
to obtain the reachable kernel trace in Linux kernel 3.14.1.

\noindent
\textbf{Results.}
To understand Lind's security properties, we need to see how much kernel is exposed to
SafePOSIX. Since our SafePOSIX is entirely built on top of the Repy sandbox kernel,
it suffices to determine the kernel trace of Repy.

\begin{table}
\centering
\scriptsize
\begin{tabular}{|l|l|l|l|l|}
  \hline
  \multirow{3}{1.5cm}{\bf Virtualization system} & \multirow{3}{0.5cm}{\bf \# of Bugs} & \multicolumn{3}{c|}{\bf Kernel trace} \\ \cline{3-5}
  & & \multirow{2}{1.5cm}{Total coverage (LOC)} & \multirow{2}{1.5cm}{In popular paths (LOC)} & \multirow{2}{1.5cm}{In risky paths (LOC)}  \\
  & & & & \\  \hline
  Repy & 1 & 74.4K & 74,4K & 0 \\
  \hline
\end{tabular}\caption{\small Reachable kernel trace analysis for Repy.}
\label{table:trace-Repy}
\end{table}

The results are shown in Table \ref{table:trace-Repy}.
The trace of Repy is slightly larger (5.8\%) than that of Lind.
This larger design does not allow attackers or bugs to
access the risky paths in the OS kernel, and it leaves open only a small amount of
additional popular paths. These are added because some functions in Repy
have more capabilities for message sending and network connection than Lind's
system call interface.
For example, in Repy,
\texttt{sendmessage()} and \texttt{openconnection()}
functions could reach out to more lines of code when fuzzed. However, the kernel
trace of Repy still lies completely within the popular kernel paths.
Since the popular paths contain fewer kernel bugs, the Repy sandbox kernel
has only a very slim chance of triggering OS kernel bugs.

Since it is the direct point of contact with the OS kernel, in theory, the Repy
 sandbox kernel could be a weakness in the overall security coverage provided by Lind.
However, the results above show that, even if it has a
bug or failure, it should not substantially increase the risk of triggering
kernel bugs.

\subsection{Performance Overhead}
\label{Performance-Evaluation}

%\textbf{Test Purpose.}
%To measure Lind's runtime performance overhead compared to Native Linux
% when running real-world applications.
%Note that the performance of Lind was not optimized in any way before running
%these tests.

\noindent
\textbf{Setup.}
%To test the execution time overhead in Lind for running real-world applications,
To measure Lind's runtime performance overhead compared to Native Linux
 when running real-world applications,
we first compiled and ran six widely-used legacy applications:
a prime number calculator Primes 1.0,
GNU Grep 2.9, GNU Wget 1.13, GNU Coreutils 8.9,
GNU Netcat 0.7.1, and K\&R Cat. Note that the performance of Lind
was not optimized in any way before running these tests.

All applications ran unaltered and correctly in Lind. To run the applications,
it was sufficient to just recompile the unmodified
source code using NaCl's compiler and Lind's \texttt{glibc} to call
into SafePOSIX.

In addition, we ran two large legacy applications, Tor 0.2.3 and Apache 2.0.64 in Lind.
%Both Tor and Apache were recompiled with NaCl's compiler and ran in Lind.
We used Tor's built-in benchmark program and Apache's benchmarking tool \texttt{ab} to perform
basic testing operations and record the execution time.

\begin{table}
\centering
\scriptsize
\begin{tabular}{|r|r|r|r|}
  \hline
  {\bf Application} & {\bf Native Code} & {\bf Lind} & {\bf Impact}  \\
  \hline
  Primes & 10000 ms & 10600 ms & 1.06x \\
  GNU Grep & 65 ms & 260 ms & 4.00x \\
  GNU Wget & 25 ms & 96 ms & 3.84x \\
  GNU Coreutils & 275 ms & 920 ms & 3.35x \\
  GNU Netcat & 780 ms & 2180 ms & 2.79x \\
  K\&R Cat & 20 ms & 125 ms & 6.25x \\
  \hline
\end{tabular}
\caption{\small Execution time performance results for six real-world applications: Native
Linux vs. Lind.}
\label{table:performance_apps}
\end{table}

\noindent
\textbf{Results.}
Table \ref{table:performance_apps} shows the runtime performance
for the six real-world applications mentioned above.
The Primes application run in Lind has a 6\% performance overhead compared to
Native Linux. CPU bound applications, like the Primes, engender little overhead,
because they run only in the NaCl sandbox. No system calls are
required, and there is no need to go through the SafePOSIX interface. The small
 amount of overhead is generated by NaCl's instruction alignment at building time.
  Another contributor to the overhead is that the instructions built by NaCl
  have a higher rate of cache misses, which can slowdown the
program. We expect other CPU bound processes to behave similarly.

The other five applications experienced slowdowns roughly ranging from 3x to 6x.
All of them require repeated calls into SafePOSIX, and this additional SafePOSIX
computation produced the additional overhead. 
The fact that the total execution time was limited 
to the magnitude of 10,000 ms ensures that the 
user experience is still reasonably efficient.

\begin{table}
\centering
\scriptsize
\begin{tabular}{|r|r|r|r|}
  \hline
  {\bf Benchmark} & {\bf Native Code} & {\bf Lind} & {\bf Impact}  \\
  \hline
  Digest Tests: & & & \\
  Set & 54.80 nsec/element & 176.86 nsec/element & 3.22x \\
  Get & 42.30 nsec/element & 134.38 nsec/element & 3.17x \\
  Add & 11.69 nsec/element & 53.91 nsec/element & 4.61x \\
  IsIn & 8.24 nsec/element & 39.82 nsec/element & 4.83x \\
  \hline
  AES Tests: & & & \\
  1 Byte & 14.83 nsec/B & 36.93 nsec/B & 2.49x \\
  16 Byte & 7.45 nsec/B & 16.95 nsec/B & 2.28x \\
  1024 Byte & 6.91 nsec/B & 15.42 nsec/B & 2.23x \\
  4096 Byte & 6.96 nsec/B & 15.35 nsec/B & 2.21x \\
  8192 Byte & 6.94 nsec/B & 15.47 nsec/B & 2.23x \\
  Cell Sized & 6.81 nsec/B & 14.71 nsec/B & 2.16x \\
  \hline
  Cell Processing: & & & \\
  Inbound & 3378.18 nsec/cell & 8418.03 nsec/cell & 2.49x \\
  (per Byte) & 6.64 nsec/B & 16.54 nsec/B & - \\
  Outbound & 3384.01 nsec/cell & 8127.42 nsec/cell & 2.40x \\
  (per Byte) & 6.65 nsec/B & 15.97 nsec/B & - \\
  \hline
\end{tabular}
\caption{\small Performance results on Tor's built-in benchmark program: Native
Linux vs. Lind.}
\label{table:performance_tor}
\end{table}

A summary of the results for Tor is shown in Table \ref{table:performance_tor}. The
benchmarks focus on cryptographic operations,
which are CPU intensive, but also make system calls like \texttt{getpid}, and reads to
\texttt{/dev/urandom}.
The digest operations time the access of a map of message digests.
The AES operations time includes encryptions of several sizes and creation of
message digests. Cell processing executes full packet encryption and decryption. In our
test, Lind slowed down these operations by 2.5x to 5x. We believe these
slowdowns are due to the increased code size produced by NaCl,
%\cappos{I'm not sure why this would be.  Does NaCl show this too?}
and the increased overhead from Lind's SafePOSIX system call interface.

Results for the Apache benchmarking tool \texttt{ab} is presented in Table \ref{table:performance_apache}.
In the set of experiments, Lind produced performance slowdowns around 2.7x.
Most of the overhead was incurred due to system call operations inside the
SafePOSIX re-creation.

As shown above, Lind generally incurs some performance overhead.
However, performance slowdown is common in virtualization systems.
For example, Graphene \cite{Graphene-14} also shows an overhead ranged from
1.4x to 2x when running applications such as Apache and UNIX benchmark.
Lind shares the same magnitude of slowdown with Graphene in many cases.
Since an attack on the kernel can have devastating
consequences, %at this initial stage,
a tradeoff between security and performance could be justified.
The fact that Lind has shown an ability to run
%\cappos{4 applications isn't many...
%Do we have Apache numbers or something else to quantify?}
%\yiwen{I am looking at more apps and libs that we can run and test in Lind.}
legacy applications
suggests that it is worth continuing research to optimize these systems.

\lois{Justin...were we missing something in this section?}
\begin{table}
\centering
\scriptsize
\begin{tabular}{|r|r|r|r|}
  \hline
  {\bf \# of Requests} & {\bf Native Code} & {\bf Lind} & {\bf Impact}  \\
  \hline
  10 & 900 ms & 2400 ms & 2.67x \\
  20 & 1700 ms & 4700 ms & 2.76x \\
  50 & 4600 ms & 13000 ms & 2.83x\\
  100 & 10000 ms & 27000 ms & 2.70x\\
  \hline
\end{tabular}
\caption{\small Performance results on Apache benchmarking tool \texttt{ab}: Native
Linux vs. Lind.}
\label{table:performance_apache}
\end{table}
